{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AMALITECH-PC-11075:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e4497996a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the SparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the songs data from the CSV file\n",
    "songs = spark.read.csv('../data/songs/songs.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
      "| id|            track_id|             artists|          album_name|          track_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
      "|  0|5SuOikwiRyPMVoIQD...|         Gen Hoshino|              Comedy|              Comedy|        73|     230666|   False|       0.676| 0.461|  1|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|           4.0|   acoustic|\n",
      "|  1|4qPNDBW1i3p13qLCt...|        Ben Woodward|    Ghost (Acoustic)|    Ghost - Acoustic|        55|     149610|   False|        0.42| 0.166|  1| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|           4.0|   acoustic|\n",
      "|  2|1iJBSr7s7jYXzM8EG...|Ingrid Michaelson...|      To Begin Again|      To Begin Again|        57|     210826|   False|       0.438| 0.359|  0|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|           4.0|   acoustic|\n",
      "|  3|6lfxq3CG4xtTiEg7o...|        Kina Grannis|Crazy Rich Asians...|Can't Help Fallin...|        71|     201933|   False|       0.266|0.0596|  0| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.0|   acoustic|\n",
      "|  4|5vjLSffimiIP26QG5...|    Chord Overstreet|             Hold On|             Hold On|        82|     198853|   False|       0.618| 0.443|  2|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949|           4.0|   acoustic|\n",
      "|  5|01MVOl9KtVTNfFiBU...|        Tyrone Wells|Days I Will Remember|Days I Will Remember|        58|     214240|   False|       0.688| 0.481|  6|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017|           4.0|   acoustic|\n",
      "|  6|6Vc5wAMmXdKIAM7WU...|A Great Big World...|Is There Anybody ...|       Say Something|        74|     229400|   False|       0.407| 0.147|  2|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|           3.0|   acoustic|\n",
      "|  7|1EzrEOXmMH3G43AXT...|          Jason Mraz|We Sing. We Dance...|           I'm Yours|        80|     242946|   False|       0.703| 0.444| 11|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|           4.0|   acoustic|\n",
      "|  8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|     189613|   False|       0.625| 0.414|  0|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|           4.0|   acoustic|\n",
      "|  9|7k9GuJYLp2AzqokyE...|      Ross Copperman|              Hunger|              Hunger|        56|     205594|   False|       0.442| 0.632|  1|   -6.77|   1|     0.0295|       0.426|         0.00419|  0.0735|  0.196| 78.899|           4.0|   acoustic|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the songs data from the CSV file\n",
    "users = spark.read.csv('../data/users/users.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+-------------+----------+\n",
      "|user_id|        user_name|user_age| user_country|created_at|\n",
      "+-------+-----------------+--------+-------------+----------+\n",
      "|      1|     Norma Fisher|      65|United States|2024-02-07|\n",
      "|      2|   Jorge Sullivan|      28|United States|2024-11-28|\n",
      "|      3|  Elizabeth Woods|      19|United States|2024-11-16|\n",
      "|      4|     Susan Wagner|      45|United States|2024-06-14|\n",
      "|      5| Peter Montgomery|      61|United States|2024-07-24|\n",
      "|      6| Theodore Mcgrath|      58|United States|2024-12-12|\n",
      "|      7|Stephanie Collins|      68|United States|2024-04-16|\n",
      "|      8| Stephanie Sutton|      53|United States|2024-05-04|\n",
      "|      9|   Brian Hamilton|      34|United States|2024-09-15|\n",
      "|     10|       Susan Levy|      18|United States|2024-09-08|\n",
      "+-------+-----------------+--------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the streams data from the CSV file\n",
    "streams1 = spark.read.csv('../data/streams/streams1.csv', header=True, inferSchema=True)\n",
    "streams2 = spark.read.csv('../data/streams/streams2.csv', header=True, inferSchema=True)\n",
    "streams3 = spark.read.csv('../data/streams/streams3.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+\n",
      "|user_id|            track_id|        listen_time|\n",
      "+-------+--------------------+-------------------+\n",
      "|  26213|4dBa8T7oDV9WvGr7k...|2024-06-25 17:43:13|\n",
      "|   6937|4osgfFTICMkcGbbig...|2024-06-25 07:26:00|\n",
      "|  21407|2LoQWx41KeqOrSFra...|2024-06-25 13:25:26|\n",
      "|  47146|7cfG5lFeJWEgpSnub...|2024-06-25 18:17:50|\n",
      "|  38594|6tilCYbheGMHo3Hw4...|2024-06-25 17:33:21|\n",
      "|  14209|2QuOheWJqShIBIYC1...|2024-06-25 02:52:20|\n",
      "|  26986|6qBSGvyUzqNQv8Xtn...|2024-06-25 22:32:51|\n",
      "|   8173|1wXSL0SAzd7mX0LM8...|2024-06-25 11:59:10|\n",
      "|  12950|0L7Nv6ToXLRAWId4e...|2024-06-25 17:54:30|\n",
      "|   2898|7tnE9vy6FCRtbZql5...|2024-06-25 18:30:31|\n",
      "+-------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat the streams data\n",
    "streams = streams1.union(streams2).union(streams3)\n",
    "streams.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the first track_id column to avoid ambiguity\n",
    "streams = streams.withColumnRenamed(\"track_id\", \"track_id_1\")\n",
    "streams = streams.withColumnRenamed(\"user_id\", \"user_id_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|         user_id_1|          track_id_1|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|             34038|               34038|\n",
      "|   mean|24934.808420001176|                NULL|\n",
      "| stddev|14444.125256146368|                NULL|\n",
      "|    min|                 3|0000vdREvCVMxbQTk...|\n",
      "|    max|             49999|7zxpdh3EqMq2JCkOI...|\n",
      "+-------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streams.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+------------------+--------------------+------------------+-----------------+--------------------+-----------------+------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+\n",
      "|summary|               id|            track_id|           artists|                  album_name|    track_name|        popularity|         duration_ms|          explicit|     danceability|              energy|              key|          loudness|            mode|       speechiness|       acousticness|  instrumentalness|         liveness|            valence|            tempo|    time_signature|       track_genre|\n",
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+------------------+--------------------+------------------+-----------------+--------------------+-----------------+------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+\n",
      "|  count|            89741|               89741|             89740|                       89740|         89740|             89741|               89741|             89741|            89741|               89741|            89741|             89741|           89741|             89741|              89741|             89741|            89741|              89741|            89741|             89741|             89741|\n",
      "|   mean|53479.14414815971|                NULL|          4425.775|                    Infinity|      Infinity|33.218531370995635|   228968.4900229639|164008.09174311926|39.77311613439367|   4.044996745550605| 7.12314423596407|2.0542242905856964|2.89385829386527|1.3271324102655435|0.32724658385956845|1.9883775972234554|3.377600199566883|0.46930670386676754|121.9193701217944|3.9940354443453945|23.083248458333333|\n",
      "| stddev|33409.98150200114|                NULL|14461.681191922153|                         NaN|           NaN|20.580922483320293|  113062.49229751206| 158943.2551850072| 3137.40901002627|   729.6975223646102|552.2772938129276|1942.6967524844183| 681.31219251588| 372.9886660028764| 0.3571049294161017| 543.4487205040567|692.9101724595571| 0.2692998643253719| 30.4185761185554| 3.347598907203215| 41.50450513962588|\n",
      "|    min|                0|0000vdREvCVMxbQTk...|            !nvite|        ! ! ! ! ! Whisper...| Live in Odeon|             Act 1|                Aida|              Aida|             Aida|                Aida|        Amneris)\"|              Aida|         Amneris|           Amneris|         Amonasro)\"|           -15.988|          -11.763|            -16.853|           -16.92|               0.0|           -12.753|\n",
      "|    max|           113999|7zz7iNGIWhmfFE7zl...|         龍藏Ryuzo|당신이 잠든 사이에 Pt. 4 ...| 행복하길 바래|           Zendani|The Song of the G...|              True|            False|Hearts And Soul -...|            False|             False|           False|             False|              False|          162800.0|            False|              False|          243.372|           182.016|       world-music|\n",
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+------------------+--------------------+------------------+-----------------+--------------------+-----------------+------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+------------------+-------------+\n",
      "|summary|          user_id|     user_name|          user_age| user_country|\n",
      "+-------+-----------------+--------------+------------------+-------------+\n",
      "|  count|            50000|         50000|             50000|        50000|\n",
      "|   mean|          25000.5|          NULL|          43.56998|         NULL|\n",
      "| stddev|14433.90106658626|          NULL|14.996324902949087|         NULL|\n",
      "|    min|                1|Aaron Alvarado|                18|    Australia|\n",
      "|    max|            50000|   Zoe Walters|                69|United States|\n",
      "+-------+-----------------+--------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = streams.join(users, streams.user_id_1 == users.user_id, how='left').join(songs, streams.track_id_1 == songs.track_id, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 31) (AMALITECH-PC-11075 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 31) (AMALITECH-PC-11075 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the songs and users dataframes\n",
    "song_users = songs.join(users, songs.id == users.user_id, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the song_users and streams dataframes\n",
    "song_users = song_users.join(streams, song_users.id == streams.user_id_1, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+-----------------+--------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------+------------------+-------------+------------------+--------------------+\n",
      "|summary|               id|            track_id|           artists|                  album_name|    track_name|       popularity|         duration_ms|          explicit|      danceability|              energy|              key|          loudness|              mode|       speechiness|       acousticness|  instrumentalness|         liveness|           valence|             tempo|    time_signature|       track_genre|           user_id|     user_name|          user_age| user_country|         user_id_1|          track_id_1|\n",
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+-----------------+--------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------+------------------+-------------+------------------+--------------------+\n",
      "|  count|           103953|              103953|            103952|                      103952|        103952|           103953|              103953|            103953|            103953|              103953|           103953|            103953|            103953|            103953|             103953|            103953|           103953|            103953|            103953|            103953|            103953|             58402|         58402|             58402|        58402|             30171|               30171|\n",
      "|   mean|49473.85337604494|                NULL|11082.019607843138|                    Infinity|      Infinity|33.20528206659411|   229178.9347165018|168458.49579831932| 35.56131626753702|  3.5823471626920864|6.886597624535814|0.6681508716858062|2.5832024974265733|1.1583248304456908|0.32191075255199497|1.7430768584220764|2.944648203104626|0.4676009677473032|122.09796798937974|  3.99335169558358| 23.00685242307692|24213.628642854695|          NULL| 43.61982466353892|         NULL|  24176.7314639886|                NULL|\n",
      "| stddev| 33065.8379690445|                NULL|27300.464372966406|                         NaN|           NaN| 20.6147581032351|  117159.24983974952|160478.05002183764|2926.8377424386235|   677.9750494591249|513.1368494562537|1805.0157594090133| 633.0254379252821|346.55429853430974| 0.3537416515016426| 504.9350462575347|643.8042463870183|0.2677604329630485|30.335395456382148|3.3388688525344294|41.280496266847386| 14415.45116440529|          NULL|14.992234976388321|         NULL|14410.199695074225|                NULL|\n",
      "|    min|                0|0000vdREvCVMxbQTk...|            !nvite|        ! ! ! ! ! Whisper...| Live in Odeon|            Act 1|                Aida|              Aida|              Aida|                Aida|        Amneris)\"|              Aida|           Amneris|           Amneris|         Amonasro)\"|           -15.988|          -11.763|           -16.853|            -16.92|               0.0|           -12.753|                 1|Aaron Alvarado|                18|    Australia|                 3|0000vdREvCVMxbQTk...|\n",
      "|    max|           113999|7zz7iNGIWhmfFE7zl...|         龍藏Ryuzo|당신이 잠든 사이에 Pt. 4 ...| 행복하길 바래|          Zendani|The Song of the G...|              True|             False|Hearts And Soul -...|            False|             False|             False|             False|              False|          162800.0|            False|             False|           243.372|           182.016|       world-music|             50000|   Zoe Walters|                69|United States|             49999|7zxpdh3EqMq2JCkOI...|\n",
      "+-------+-----------------+--------------------+------------------+----------------------------+--------------+-----------------+--------------------+------------------+------------------+--------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------+------------------+-------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_users.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+-------+-----------------+--------+-------------+----------+---------+--------------------+-------------------+\n",
      "| id|            track_id|             artists|          album_name|          track_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|user_id|        user_name|user_age| user_country|created_at|user_id_1|          track_id_1|        listen_time|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+-------+-----------------+--------+-------------+----------+---------+--------------------+-------------------+\n",
      "|  0|5SuOikwiRyPMVoIQD...|         Gen Hoshino|              Comedy|              Comedy|        73|     230666|   False|       0.676| 0.461|  1|  -6.746|   0|      0.143|      0.0322|         1.01E-6|   0.358|  0.715| 87.917|           4.0|   acoustic|   NULL|             NULL|    NULL|         NULL|      NULL|     NULL|                NULL|               NULL|\n",
      "|  1|4qPNDBW1i3p13qLCt...|        Ben Woodward|    Ghost (Acoustic)|    Ghost - Acoustic|        55|     149610|   False|        0.42| 0.166|  1| -17.235|   1|     0.0763|       0.924|         5.56E-6|   0.101|  0.267| 77.489|           4.0|   acoustic|      1|     Norma Fisher|      65|United States|2024-02-07|     NULL|                NULL|               NULL|\n",
      "|  2|1iJBSr7s7jYXzM8EG...|Ingrid Michaelson...|      To Begin Again|      To Begin Again|        57|     210826|   False|       0.438| 0.359|  0|  -9.734|   1|     0.0557|        0.21|             0.0|   0.117|   0.12| 76.332|           4.0|   acoustic|      2|   Jorge Sullivan|      28|United States|2024-11-28|     NULL|                NULL|               NULL|\n",
      "|  3|6lfxq3CG4xtTiEg7o...|        Kina Grannis|Crazy Rich Asians...|Can't Help Fallin...|        71|     201933|   False|       0.266|0.0596|  0| -18.515|   1|     0.0363|       0.905|         7.07E-5|   0.132|  0.143| 181.74|           3.0|   acoustic|      3|  Elizabeth Woods|      19|United States|2024-11-16|        3|17OzwixEJc4MEjKjF...|2024-06-25 08:46:10|\n",
      "|  4|5vjLSffimiIP26QG5...|    Chord Overstreet|             Hold On|             Hold On|        82|     198853|   False|       0.618| 0.443|  2|  -9.681|   1|     0.0526|       0.469|             0.0|  0.0829|  0.167|119.949|           4.0|   acoustic|      4|     Susan Wagner|      45|United States|2024-06-14|     NULL|                NULL|               NULL|\n",
      "|  5|01MVOl9KtVTNfFiBU...|        Tyrone Wells|Days I Will Remember|Days I Will Remember|        58|     214240|   False|       0.688| 0.481|  6|  -8.807|   1|      0.105|       0.289|             0.0|   0.189|  0.666| 98.017|           4.0|   acoustic|      5| Peter Montgomery|      61|United States|2024-07-24|     NULL|                NULL|               NULL|\n",
      "|  6|6Vc5wAMmXdKIAM7WU...|A Great Big World...|Is There Anybody ...|       Say Something|        74|     229400|   False|       0.407| 0.147|  2|  -8.822|   1|     0.0355|       0.857|         2.89E-6|  0.0913| 0.0765|141.284|           3.0|   acoustic|      6| Theodore Mcgrath|      58|United States|2024-12-12|        6|5uraJqtCBvLpwt3Ve...|2024-06-25 05:27:49|\n",
      "|  7|1EzrEOXmMH3G43AXT...|          Jason Mraz|We Sing. We Dance...|           I'm Yours|        80|     242946|   False|       0.703| 0.444| 11|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|           4.0|   acoustic|      7|Stephanie Collins|      68|United States|2024-04-16|     NULL|                NULL|               NULL|\n",
      "|  8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|     189613|   False|       0.625| 0.414|  0|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|           4.0|   acoustic|      8| Stephanie Sutton|      53|United States|2024-05-04|        8|30WZViwU19Gq6aKQ1...|2024-06-25 21:32:20|\n",
      "|  8|0IktbUcnAGrvD03AW...|Jason Mraz;Colbie...|We Sing. We Dance...|               Lucky|        74|     189613|   False|       0.625| 0.414|  0|    -8.7|   1|     0.0369|       0.294|             0.0|   0.151|  0.669|130.088|           4.0|   acoustic|      8| Stephanie Sutton|      53|United States|2024-05-04|        8|7Gpr3kKk4BMgItz6U...|2024-06-25 21:32:23|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+-------+-----------------+--------+-------------+----------+---------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_users.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'track_id',\n",
       " 'artists',\n",
       " 'album_name',\n",
       " 'track_name',\n",
       " 'popularity',\n",
       " 'duration_ms',\n",
       " 'explicit',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'key',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'time_signature',\n",
       " 'track_genre',\n",
       " 'user_id',\n",
       " 'user_name',\n",
       " 'user_age',\n",
       " 'user_country',\n",
       " 'created_at',\n",
       " 'user_id_1',\n",
       " 'track_id_1',\n",
       " 'listen_time']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|track_genre|      date|listen_count|\n",
      "+-----------+----------+------------+\n",
      "|   acoustic|2024-05-20|           7|\n",
      "|   acoustic|2024-08-13|           2|\n",
      "|   afrobeat|2024-11-23|           2|\n",
      "|   alt-rock|2024-08-26|           3|\n",
      "|   alt-rock|2024-09-25|           7|\n",
      "|   alt-rock|2024-06-03|           4|\n",
      "|alternative|2024-10-31|           9|\n",
      "|alternative|2024-09-04|           1|\n",
      "|alternative|2024-02-11|           2|\n",
      "|alternative|2024-01-18|           1|\n",
      "|    ambient|2024-11-06|          10|\n",
      "|    ambient|2024-08-08|           5|\n",
      "|      anime|2024-11-02|           5|\n",
      "|      anime|2024-04-09|           9|\n",
      "|black-metal|2024-04-10|           6|\n",
      "|  bluegrass|2024-03-14|           2|\n",
      "|  bluegrass|2024-04-22|           3|\n",
      "|    british|2024-10-11|           3|\n",
      "|   cantopop|2024-02-26|           6|\n",
      "|   cantopop|2024-04-11|           1|\n",
      "+-----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listen_count = song_users.groupBy(\"track_genre\", F.to_date(\"created_at\").alias(\"date\")) \\\n",
    "                 .agg(F.count(\"track_id\").alias(\"listen_count\"))\n",
    "listen_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'track_id',\n",
       " 'artists',\n",
       " 'album_name',\n",
       " 'track_name',\n",
       " 'popularity',\n",
       " 'duration_ms',\n",
       " 'explicit',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'key',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'time_signature',\n",
       " 'track_genre',\n",
       " 'user_id',\n",
       " 'user_name',\n",
       " 'user_age',\n",
       " 'user_country',\n",
       " 'created_at',\n",
       " 'user_id_1',\n",
       " 'track_id_1',\n",
       " 'listen_time']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------+\n",
      "|track_genre|      date|unique_listeners|\n",
      "+-----------+----------+----------------+\n",
      "|  bluegrass|2024-04-22|               1|\n",
      "|      anime|2024-11-02|               3|\n",
      "|   alt-rock|2024-09-25|               2|\n",
      "|   afrobeat|2024-11-23|               1|\n",
      "|     comedy|2024-02-26|               2|\n",
      "|      anime|2024-04-09|               2|\n",
      "|   acoustic|2024-05-20|               3|\n",
      "|      disco|2024-09-23|               2|\n",
      "|   children|2024-02-03|               1|\n",
      "|    ambient|2024-08-08|               3|\n",
      "|black-metal|2024-04-10|               1|\n",
      "|  bluegrass|2024-03-14|               1|\n",
      "|  classical|2024-03-12|               1|\n",
      "|      dance|2024-04-11|               1|\n",
      "| deep-house|2024-08-04|               1|\n",
      "|   cantopop|2024-02-26|               2|\n",
      "|    ambient|2024-11-06|               2|\n",
      "|alternative|2024-10-31|               3|\n",
      "|   acoustic|2024-08-13|               1|\n",
      "|      chill|2024-11-07|               1|\n",
      "+-----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_listeners = song_users.groupBy(\"track_genre\", F.to_date(\"created_at\").alias(\"date\")) \\\n",
    "                     .agg(F.countDistinct(\"user_id_1\").alias(\"unique_listeners\"))\n",
    "unique_listeners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|track_genre|      date|total_listening_time|\n",
      "+-----------+----------+--------------------+\n",
      "|   acoustic|2024-05-20|           1549666.0|\n",
      "|   acoustic|2024-08-13|            288280.0|\n",
      "|   afrobeat|2024-11-23|            782559.0|\n",
      "|   alt-rock|2024-08-26|            637030.0|\n",
      "|   alt-rock|2024-09-25|           1702426.0|\n",
      "|   alt-rock|2024-06-03|            948265.0|\n",
      "|alternative|2024-10-31|           2189045.0|\n",
      "|alternative|2024-09-04|            170771.0|\n",
      "|alternative|2024-02-11|            287121.0|\n",
      "|alternative|2024-01-18|            234760.0|\n",
      "|    ambient|2024-11-06|           2821263.0|\n",
      "|    ambient|2024-08-08|           1195224.0|\n",
      "|      anime|2024-11-02|            724924.0|\n",
      "|      anime|2024-04-09|           2037219.0|\n",
      "|black-metal|2024-04-10|           1326182.0|\n",
      "|  bluegrass|2024-03-14|            419593.0|\n",
      "|  bluegrass|2024-04-22|            219000.0|\n",
      "|    british|2024-10-11|            611801.0|\n",
      "|   cantopop|2024-02-26|           1455488.0|\n",
      "|   cantopop|2024-04-11|            302733.0|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_listening_time = song_users.groupBy(\"track_genre\", F.to_date(\"created_at\").alias(\"date\")) \\\n",
    "                         .agg(F.sum(\"duration_ms\").alias(\"total_listening_time\"))\n",
    "total_listening_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------------+---------------------------+\n",
      "|track_genre|      date|total_listening_time|unique_listeners|avg_listening_time_per_user|\n",
      "+-----------+----------+--------------------+----------------+---------------------------+\n",
      "|  bluegrass|2024-04-22|            219000.0|               1|                   219000.0|\n",
      "|      anime|2024-11-02|            724924.0|               3|         241641.33333333334|\n",
      "|   alt-rock|2024-09-25|           1702426.0|               2|                   851213.0|\n",
      "|   afrobeat|2024-11-23|            782559.0|               1|                   782559.0|\n",
      "|     comedy|2024-02-26|           1303957.0|               2|                   651978.5|\n",
      "|      anime|2024-04-09|           2037219.0|               2|                  1018609.5|\n",
      "|   acoustic|2024-05-20|           1549666.0|               3|          516555.3333333333|\n",
      "|      disco|2024-09-23|           1162248.0|               2|                   581124.0|\n",
      "|   children|2024-02-03|            176638.0|               1|                   176638.0|\n",
      "|    ambient|2024-08-08|           1195224.0|               3|                   398408.0|\n",
      "|black-metal|2024-04-10|           1326182.0|               1|                  1326182.0|\n",
      "|  bluegrass|2024-03-14|            419593.0|               1|                   419593.0|\n",
      "|  classical|2024-03-12|            538719.0|               1|                   538719.0|\n",
      "|      dance|2024-04-11|           1017574.0|               1|                  1017574.0|\n",
      "| deep-house|2024-08-04|            746974.0|               1|                   746974.0|\n",
      "|   cantopop|2024-02-26|           1455488.0|               2|                   727744.0|\n",
      "|    ambient|2024-11-06|           2821263.0|               2|                  1410631.5|\n",
      "|alternative|2024-10-31|           2189045.0|               3|          729681.6666666666|\n",
      "|   acoustic|2024-08-13|            288280.0|               1|                   288280.0|\n",
      "|      chill|2024-11-07|            656704.0|               1|                   656704.0|\n",
      "+-----------+----------+--------------------+----------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_listening_time_per_user = total_listening_time.join(unique_listeners, [\"track_genre\", \"date\"]) \\\n",
    "                                                  .withColumn(\"avg_listening_time_per_user\", \n",
    "                                                              F.col(\"total_listening_time\") / F.col(\"unique_listeners\"))\n",
    "avg_listening_time_per_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+------------+----+\n",
      "|      date|          track_name|track_genre|listen_count|rank|\n",
      "+----------+--------------------+-----------+------------+----+\n",
      "|      NULL|Rockin' Around Th...| rockabilly|          45|   1|\n",
      "|      NULL|Little Saint Nick...| psych-rock|          38|   2|\n",
      "|      NULL|Let It Snow! Let ...|       jazz|          32|   3|\n",
      "|2024-06-25|     Run Rudolph Run|      blues|          15|   1|\n",
      "|2024-06-25|  Frosty The Snowman|      blues|          12|   2|\n",
      "|2024-06-25|           Last Last|      dance|          10|   3|\n",
      "|2024-06-25|Cozy Little Chris...|      dance|          10|   3|\n",
      "|2024-06-25|Devil Doesn't Bar...|    electro|          10|   3|\n",
      "+----------+--------------------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute listen count per song per genre per day\n",
    "song_listen_count = song_users.groupBy( \n",
    "    F.to_date(\"listen_time\").alias(\"date\"), \n",
    "    \"track_name\",\n",
    "    \"track_genre\"\n",
    ").agg(F.countDistinct(\"track_id\").alias(\"listen_count\"))\n",
    "\n",
    "# Step 3: Define ranking window for top songs per genre per day\n",
    "song_rank_window = Window.partitionBy(\"date\").orderBy(F.desc(\"listen_count\"))\n",
    "\n",
    "# Step 4: Rank songs and filter for the top 3 per genre per day\n",
    "top_songs_per_genre = song_listen_count.withColumn(\"rank\", F.rank().over(song_rank_window)) \\\n",
    "                                     .filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "# Step 5: Show results\n",
    "top_songs_per_genre.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+----+\n",
      "|  track_genre|      date|listen_count|rank|\n",
      "+-------------+----------+------------+----+\n",
      "|        tango|      NULL|         999|   1|\n",
      "|        study|      NULL|         998|   2|\n",
      "|  heavy-metal|      NULL|         997|   3|\n",
      "|        sleep|      NULL|         996|   4|\n",
      "|       j-idol|      NULL|         995|   5|\n",
      "|     cantopop|2024-06-25|         378|   1|\n",
      "|        forro|2024-06-25|         377|   2|\n",
      "|chicago-house|2024-06-25|         375|   3|\n",
      "|       disney|2024-06-25|         373|   4|\n",
      "|        chill|2024-06-25|         372|   5|\n",
      "+-------------+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Ensure created_at exists and is correctly formatted\n",
    "song_users = song_users.withColumn(\"created_at\", F.to_timestamp(\"created_at\"))\n",
    "\n",
    "# Step 1: Compute listen count per genre per day\n",
    "genre_listen_count = song_users.groupBy(\n",
    "    \"track_genre\",\n",
    "    F.to_date(\"listen_time\").alias(\"date\")  # Ensure conversion to date format\n",
    ").agg(F.countDistinct(\"track_id\").alias(\"listen_count\"))\n",
    "\n",
    "# Step 2: Define a ranking window for top genres per day\n",
    "genre_rank_window = Window.partitionBy(\"date\").orderBy(F.desc(\"listen_count\"))\n",
    "\n",
    "# Step 3: Apply ranking and filter for the top 5 genres per day\n",
    "top_genres_per_day = genre_listen_count.withColumn(\"rank\", F.rank().over(genre_rank_window)) \\\n",
    "                                       .filter(F.col(\"rank\") <= 5)\n",
    "top_genres_per_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------+----------------+--------------------+---------------------------+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "|created_date|track_genre   |listen_count|unique_listeners|total_listening_time|avg_listening_time_per_user|top_3_songs                                                                                                             |top_5_genres                                                                                |\n",
      "+------------+--------------+------------+----------------+--------------------+---------------------------+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "|2024-08-07  |breakbeat     |6           |6               |1654428.0           |275.738                    |Black & Blue, Under The Influence, Umbrella, Cyclone, Take My Love For Granted, Bermuda                                 |disney, bluegrass, grunge, british, ambient, electronic                                     |\n",
      "|2024-07-28  |dance         |3           |3               |632373.0            |210.791                    |F It Up, break up with your girlfriend, i'm bored, Best Life                                                            |disco, black-metal, breakbeat, anime, afrobeat                                              |\n",
      "|2024-08-01  |chill         |6           |6               |680055.0            |113.3425                   |Sad!, Rain on Me, Reasons You Should Care, Scars, stay with me, Will You Stay Here with Me                              |anime, french, edm, afrobeat, alt-rock, forro                                               |\n",
      "|2024-11-02  |british       |6           |3               |1489212.0           |496.404                    |Second Suite in F Major, Op. 28 No. 2, H. 106: I. March, Easy On Me, I Feel Fine - Remastered 2015                      |comedy, detroit-techno, alt-rock, bluegrass, grindcore, happy                               |\n",
      "|2024-03-29  |chill         |4           |4               |911345.0            |227.83625                  |summer nights, seasons, Your Dog Loves You (Feat. Crush)                                                                |chicago-house, forro, happy, country, deep-house, dance                                     |\n",
      "|2024-11-27  |dancehall     |7           |4               |1452133.0           |363.03325                  |My Story, Mek It Bunx Up (feat. Marcy Chin), Oliver Twist, Ozumba Mbadiwe - Remix                                       |dancehall, chill, chicago-house, anime, disney                                              |\n",
      "|2024-09-30  |brazil        |2           |2               |534413.0            |267.2065                   |Toca em Mim de Novo - Ao Vivo, Saudação A Yamanjà                                                                       |bluegrass, grunge, cantopop, black-metal, chill, deep-house, dance, dub, electro, electronic|\n",
      "|2024-10-14  |breakbeat     |2           |2               |822672.0            |411.336                    |Electronic Battle Weapon 2, Lava Flow                                                                                   |forro, classical, bluegrass, gospel, dance, drum-and-bass                                   |\n",
      "|2024-07-02  |dancehall     |2           |2               |340132.0            |170.066                    |Slow Motion, Paranoia - Self Protection P2                                                                              |chill, hard-rock, black-metal, anime, dub                                                   |\n",
      "|2024-01-23  |detroit-techno|8           |3               |2660879.0           |886.9596666666666          |S.E.X - A.O.L Remix, Do You Love What You Feel - Duane Bradley Album Mix, Believer [featuring Ann Saunderson] - The Edit|chill, detroit-techno, grindcore, disco, french                                             |\n",
      "|2024-05-24  |chill         |4           |3               |570403.0            |190.13433333333333         |Smile, Atlantis (Super Sped Up), Older                                                                                  |dance, black-metal, edm, club, deep-house, happy, grindcore                                 |\n",
      "|2024-09-03  |ambient       |4           |4               |1208791.0           |302.19775                  |Abiogenesis, Haul - Radio Edit, Dealing With Destruction - From “Chernobyl” TV Series Soundtrack, Pashupati             |afrobeat, alt-rock, groove, dance, garage                                                   |\n",
      "|2024-12-26  |anime         |2           |2               |418866.0            |209.433                    |FLY HIGH!!, Obsolete                                                                                                    |hardcore, deep-house, detroit-techno, emo, drum-and-bass, disney                            |\n",
      "|2024-12-05  |comedy        |1           |1               |82680.0             |82.68                      |An Importanta Relationship Lesson                                                                                       |bluegrass, french, disco, black-metal, cantopop, classical, guitar, dub                     |\n",
      "|2024-05-18  |country       |1           |1               |219169.0            |219.169                    |Cuddle Up, Cozy Down Christmas                                                                                          |afrobeat, goth, disney, deep-house, emo                                                     |\n",
      "|2024-07-11  |british       |5           |3               |1014413.0           |338.1376666666667          |I Should Have Known Better - Remastered 2009, Am I Forgiven?, Run                                                       |anime, ambient, brazil, electro, alt-rock, guitar                                           |\n",
      "|2024-02-02  |detroit-techno|1           |1               |178432.0            |178.432                    |Aqua Jujidsu                                                                                                            |brazil, chicago-house, comedy, ambient, emo                                                 |\n",
      "|2024-03-23  |club          |3           |3               |613153.0            |204.38433333333333         |Sector, Reset, She                                                                                                      |bluegrass, happy, cantopop, detroit-techno, chill                                           |\n",
      "|2024-11-03  |ambient       |7           |5               |2458596.0           |491.7192                   |Weightless, What Gently Flutters, Shiver, Viva Vida Amor - Solo Piano Version, Flying Carpet                            |brazil, guitar, afrobeat, dance, ambient, club, children                                    |\n",
      "|2024-01-10  |anime         |6           |5               |1141817.0           |228.36339999999998         |Voices of the Chord, Gurenge Lofi (Demon Slayer), オレンジ, swordland, Yellow Flash (Minato Rap)                        |black-metal, anime, breakbeat, cantopop, afrobeat, guitar                                   |\n",
      "+------------+--------------+------------+----------------+--------------------+---------------------------+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a date column\n",
    "song_users = song_users.withColumn(\"created_date\", F.to_date(\"created_at\"))\n",
    "\n",
    "# 1. Daily Genre-Level KPIs\n",
    "# Listen count, unique listeners, total listening time, average listening time per user\n",
    "kpis = song_users.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.count(\"track_id\").alias(\"listen_count\"),\n",
    "    F.countDistinct(\"user_id\").alias(\"unique_listeners\"),\n",
    "    F.sum(\"duration_ms\").alias(\"total_listening_time\")\n",
    ").withColumn(\n",
    "    \"avg_listening_time_per_user\", \n",
    "    F.col(\"total_listening_time\") / F.col(\"unique_listeners\") / 1000  # converting ms to seconds\n",
    ")\n",
    "\n",
    "# 2. Top 3 Songs per Genre per Day\n",
    "song_counts = song_users.groupBy(\"created_date\", \"track_genre\", \"track_name\").agg(\n",
    "    F.count(\"track_id\").alias(\"listen_count\")\n",
    ")\n",
    "\n",
    "song_rank_window = Window.partitionBy(\"created_date\", \"track_genre\").orderBy(F.desc(\"listen_count\"))\n",
    "\n",
    "ranked_songs = song_counts.withColumn(\"rank\", F.rank().over(song_rank_window)).filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "# Aggregate top 3 songs into a single string per genre per day\n",
    "top3_songs_per_genre = ranked_songs.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.concat_ws(\", \", F.collect_list(\"track_name\")).alias(\"top_3_songs\")\n",
    ")\n",
    "\n",
    "# 3. Top 5 Genres per Day\n",
    "genre_counts_per_day = song_users.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.count(\"track_id\").alias(\"genre_listen_count\")\n",
    ")\n",
    "\n",
    "genre_rank_window = Window.partitionBy(\"created_date\").orderBy(F.desc(\"genre_listen_count\"))\n",
    "\n",
    "ranked_genres = genre_counts_per_day.withColumn(\"rank\", F.rank().over(genre_rank_window)).filter(F.col(\"rank\") <= 5)\n",
    "\n",
    "# Aggregate top 5 genres into a single string per day\n",
    "top5_genres_per_day = ranked_genres.groupBy(\"created_date\").agg(\n",
    "    F.concat_ws(\", \", F.collect_list(\"track_genre\")).alias(\"top_5_genres\")\n",
    ")\n",
    "\n",
    "# Join all results\n",
    "final_kpis = kpis \\\n",
    "    .join(top3_songs_per_genre, [\"created_date\", \"track_genre\"], \"left\") \\\n",
    "    .join(top5_genres_per_day, \"created_date\", \"left\")\n",
    "\n",
    "# Show final daily genre-level KPIs\n",
    "final_kpis.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a date column\n",
    "data = data.withColumn(\"created_date\", F.to_date(\"created_at\"))\n",
    "\n",
    "# 1. Daily Genre-Level KPIs\n",
    "# Listen count, unique listeners, total listening time, average listening time per user\n",
    "kpis = data.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.count(\"track_id\").alias(\"listen_count\"),\n",
    "    F.countDistinct(\"user_id\").alias(\"unique_listeners\"),\n",
    "    F.sum(\"duration_ms\").alias(\"total_listening_time\")\n",
    ").withColumn(\n",
    "    \"avg_listening_time_per_user\", \n",
    "    F.col(\"total_listening_time\") / F.col(\"unique_listeners\") / 1000  # converting ms to seconds\n",
    ")\n",
    "\n",
    "# 2. Top 3 Songs per Genre per Day\n",
    "song_counts = data.groupBy(\"created_date\", \"track_genre\", \"track_name\").agg(\n",
    "    F.count(\"track_id\").alias(\"listen_count\")\n",
    ")\n",
    "\n",
    "song_rank_window = Window.partitionBy(\"created_date\", \"track_genre\").orderBy(F.desc(\"listen_count\"))\n",
    "\n",
    "ranked_songs = song_counts.withColumn(\"rank\", F.rank().over(song_rank_window)).filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "# Aggregate top 3 songs into a single string per genre per day\n",
    "top3_songs_per_genre = ranked_songs.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.concat_ws(\"| \", F.collect_list(\"track_name\")).alias(\"top_3_songs\")\n",
    ")\n",
    "\n",
    "# 3. Top 5 Genres per Day\n",
    "genre_counts_per_day = data.groupBy(\"created_date\", \"track_genre\").agg(\n",
    "    F.count(\"track_id\").alias(\"genre_listen_count\")\n",
    ")\n",
    "\n",
    "genre_rank_window = Window.partitionBy(\"created_date\").orderBy(F.desc(\"genre_listen_count\"))\n",
    "\n",
    "ranked_genres = genre_counts_per_day.withColumn(\"rank\", F.rank().over(genre_rank_window)).filter(F.col(\"rank\") <= 5)\n",
    "\n",
    "# Aggregate top 5 genres into a single string per day\n",
    "top5_genres_per_day = ranked_genres.groupBy(\"created_date\").agg(\n",
    "    F.concat_ws(\"| \", F.collect_list(\"track_genre\")).alias(\"top_5_genres\")\n",
    ")\n",
    "\n",
    "# Join all results\n",
    "final_kpis = kpis \\\n",
    "    .join(top3_songs_per_genre, [\"created_date\", \"track_genre\"], \"left\") \\\n",
    "    .join(top5_genres_per_day, \"created_date\", \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o416.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 240) (AMALITECH-PC-11075 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinal_kpis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HeskeyAmoakoFordjour\\anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o416.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 240) (AMALITECH-PC-11075 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "final_kpis.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------+----------------+--------------------+---------------------------+\n",
      "|created_date|   track_genre|listen_count|unique_listeners|total_listening_time|avg_listening_time_per_user|\n",
      "+------------+--------------+------------+----------------+--------------------+---------------------------+\n",
      "|  2024-01-01|        garage|           1|               1|            111986.0|                    111.986|\n",
      "|  2024-01-01|     grindcore|           1|               1|            240564.0|                    240.564|\n",
      "|  2024-01-01|    electronic|           1|               1|            178767.0|                    178.767|\n",
      "|  2024-01-01|    industrial|           2|               2|            605619.0|                   302.8095|\n",
      "|  2024-01-01|   world-music|           3|               3|            861799.0|          287.2663333333333|\n",
      "|  2024-01-01|    show-tunes|           2|               2|            385536.0|                    192.768|\n",
      "|  2024-01-01|        gospel|           3|               2|            732812.0|                    366.406|\n",
      "|  2024-01-01|     dancehall|           1|               1|            181224.0|                    181.224|\n",
      "|  2024-01-01|     metalcore|           1|               1|            205160.0|                     205.16|\n",
      "|  2024-01-01|      mandopop|           3|               3|            801890.0|          267.2966666666667|\n",
      "|  2024-01-01|          folk|           1|               1|            178573.0|                    178.573|\n",
      "|  2024-01-01|          goth|           3|               3|            688417.0|         229.47233333333335|\n",
      "|  2024-01-01|        trance|           1|               1|            489931.0|                    489.931|\n",
      "|  2024-01-01|          jazz|           1|               1|            172653.0|                    172.653|\n",
      "|  2024-01-01|           idm|           2|               2|            334972.0|                    167.486|\n",
      "|  2024-01-01|         j-pop|           5|               5|            963758.0|                   192.7516|\n",
      "|  2024-01-01|         party|           2|               2|            390279.0|                   195.1395|\n",
      "|  2024-01-01|         study|           1|               1|            147428.0|                    147.428|\n",
      "|  2024-01-01|         piano|           1|               1|            223586.0|                    223.586|\n",
      "|  2024-01-01|minimal-techno|           1|               1|            295893.0|                    295.893|\n",
      "+------------+--------------+------------+----------------+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kpis.orderBy(\"created_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a dataframe\n",
    "streams1 = pd.read_csv('../data/streams/streams1.csv')\n",
    "streams2 = pd.read_csv('../data/streams/streams2.csv')\n",
    "streams3 = pd.read_csv('../data/streams/streams3.csv')\n",
    "users_df = pd.read_csv('../data/users/users.csv')\n",
    "songs_df = pd.read_csv('../data/songs/songs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>listen_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26213</td>\n",
       "      <td>4dBa8T7oDV9WvGr7kVS4Ez</td>\n",
       "      <td>2024-06-25 17:43:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6937</td>\n",
       "      <td>4osgfFTICMkcGbbigdsa53</td>\n",
       "      <td>2024-06-25 07:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21407</td>\n",
       "      <td>2LoQWx41KeqOrSFra089YS</td>\n",
       "      <td>2024-06-25 13:25:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47146</td>\n",
       "      <td>7cfG5lFeJWEgpSnubt4O4W</td>\n",
       "      <td>2024-06-25 18:17:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38594</td>\n",
       "      <td>6tilCYbheGMHo3Hw4F22hF</td>\n",
       "      <td>2024-06-25 17:33:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                track_id          listen_time\n",
       "0    26213  4dBa8T7oDV9WvGr7kVS4Ez  2024-06-25 17:43:13\n",
       "1     6937  4osgfFTICMkcGbbigdsa53  2024-06-25 07:26:00\n",
       "2    21407  2LoQWx41KeqOrSFra089YS  2024-06-25 13:25:26\n",
       "3    47146  7cfG5lFeJWEgpSnubt4O4W  2024-06-25 18:17:50\n",
       "4    38594  6tilCYbheGMHo3Hw4F22hF  2024-06-25 17:33:21"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat the streams data\n",
    "streams = pd.concat([streams1, streams2, streams3])\n",
    "streams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the streams, users, and songs dataframes\n",
    "merged = streams.merge(users_df, on='user_id', how='left').merge(songs_df, on='track_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>listen_time</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_country</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26213</td>\n",
       "      <td>4dBa8T7oDV9WvGr7kVS4Ez</td>\n",
       "      <td>2024-06-25 17:43:13</td>\n",
       "      <td>Cathy Smith</td>\n",
       "      <td>39</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>91162</td>\n",
       "      <td>Panic! At The Disco</td>\n",
       "      <td>Pop n' Fresh</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.00230</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>117.637</td>\n",
       "      <td>4</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6937</td>\n",
       "      <td>4osgfFTICMkcGbbigdsa53</td>\n",
       "      <td>2024-06-25 07:26:00</td>\n",
       "      <td>Tommy Grant</td>\n",
       "      <td>61</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-10-16</td>\n",
       "      <td>103402</td>\n",
       "      <td>Frank Ocean</td>\n",
       "      <td>Novacane</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>93.510</td>\n",
       "      <td>4</td>\n",
       "      <td>soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21407</td>\n",
       "      <td>2LoQWx41KeqOrSFra089YS</td>\n",
       "      <td>2024-06-25 13:25:26</td>\n",
       "      <td>Garrett Ryan</td>\n",
       "      <td>34</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>42552</td>\n",
       "      <td>Internal Rot</td>\n",
       "      <td>Grieving Birth</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.881</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.71800</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>122.842</td>\n",
       "      <td>3</td>\n",
       "      <td>grindcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47146</td>\n",
       "      <td>7cfG5lFeJWEgpSnubt4O4W</td>\n",
       "      <td>2024-06-25 18:17:50</td>\n",
       "      <td>Patrick Nash</td>\n",
       "      <td>44</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>50935</td>\n",
       "      <td>Rata Blanca</td>\n",
       "      <td>The forgotten Kingdom</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.673</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.00434</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>130.064</td>\n",
       "      <td>4</td>\n",
       "      <td>heavy-metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38594</td>\n",
       "      <td>6tilCYbheGMHo3Hw4F22hF</td>\n",
       "      <td>2024-06-25 17:33:21</td>\n",
       "      <td>Bonnie Walls DVM</td>\n",
       "      <td>69</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>79664</td>\n",
       "      <td>Lafayette Leake</td>\n",
       "      <td>Easy Blues (France, 1978) [Blues Reference]</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.399</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.07230</td>\n",
       "      <td>0.0853</td>\n",
       "      <td>0.7510</td>\n",
       "      <td>110.932</td>\n",
       "      <td>4</td>\n",
       "      <td>piano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                track_id          listen_time         user_name  \\\n",
       "0    26213  4dBa8T7oDV9WvGr7kVS4Ez  2024-06-25 17:43:13       Cathy Smith   \n",
       "1     6937  4osgfFTICMkcGbbigdsa53  2024-06-25 07:26:00       Tommy Grant   \n",
       "2    21407  2LoQWx41KeqOrSFra089YS  2024-06-25 13:25:26      Garrett Ryan   \n",
       "3    47146  7cfG5lFeJWEgpSnubt4O4W  2024-06-25 18:17:50      Patrick Nash   \n",
       "4    38594  6tilCYbheGMHo3Hw4F22hF  2024-06-25 17:33:21  Bonnie Walls DVM   \n",
       "\n",
       "   user_age   user_country  created_at      id              artists  \\\n",
       "0        39  United States  2024-06-05   91162  Panic! At The Disco   \n",
       "1        61  United States  2024-10-16  103402          Frank Ocean   \n",
       "2        34  United States  2024-08-05   42552         Internal Rot   \n",
       "3        44  United States  2024-01-21   50935          Rata Blanca   \n",
       "4        69  United States  2024-11-07   79664      Lafayette Leake   \n",
       "\n",
       "                                    album_name  ... loudness  mode  \\\n",
       "0                                 Pop n' Fresh  ...   -4.032     0   \n",
       "1                                     Novacane  ...   -9.112     1   \n",
       "2                               Grieving Birth  ...   -3.881     1   \n",
       "3                        The forgotten Kingdom  ...   -3.673     0   \n",
       "4  Easy Blues (France, 1978) [Blues Reference]  ...  -11.399     1   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0313      0.001090           0.00230    0.2710   0.1790  117.637   \n",
       "1       0.0919      0.058400           0.00126    0.1600   0.3700   93.510   \n",
       "2       0.1410      0.000412           0.71800    0.5000   0.0689  122.842   \n",
       "3       0.0362      0.004220           0.00434    0.1840   0.5230  130.064   \n",
       "4       0.0307      0.237000           0.07230    0.0853   0.7510  110.932   \n",
       "\n",
       "   time_signature  track_genre  \n",
       "0               4         rock  \n",
       "1               4         soul  \n",
       "2               3    grindcore  \n",
       "3               4  heavy-metal  \n",
       "4               4        piano  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'track_id', 'listen_time', 'user_name', 'user_age',\n",
       "       'user_country', 'created_at', 'id', 'artists', 'album_name',\n",
       "       'track_name', 'popularity', 'duration_ms', 'explicit', 'danceability',\n",
       "       'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature',\n",
       "       'track_genre'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
